{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydromedusa Solmaris\n",
    "*By Jan Jetze Beitler & Sinan Ersin*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_model\n",
    "from file_handler import load_images\n",
    "from functions import cross_validation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import random\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directories\n",
    "root = '../Data/'\n",
    "train_img_dir = 'train_images/'\n",
    "train_lbl_fle = 'train_onelabel.csv'\n",
    "test_img_dir = 'test_images/'\n",
    "submission_csv = 'output.csv'\n",
    "\n",
    "# if load_size is None, all images will be loaded\n",
    "load_size = None\n",
    "img_size = 64\n",
    "batch_size = 80\n",
    "num_epochs = 200\n",
    "num_folds = 3 # used for cross validation\n",
    "\n",
    "# if float, data will be splitted with float being percentage of test data\n",
    "split_test_train = 0\n",
    "\n",
    "# parameters for cross validation. See Markdown below for instructions\n",
    "parameters = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter tuning\n",
    "Fill parameters-dict to run cross validations over parameters.\n",
    "\n",
    "- *key* is parameter to tune.\n",
    "- *value* is list of inputs for parameter.\n",
    "\n",
    "Multiple key-value pairs possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "Y, X = load_images(root + train_img_dir, \n",
    "                   labels_file= root + train_lbl_fle,\n",
    "                   num=load_size, shape=img_size)\n",
    "\n",
    "input_shape = X[0].shape\n",
    "\n",
    "# split data if specified\n",
    "if split_test_train:\n",
    "    sample = random.sample(range(len(Y)), int(len(X) * split_test_train))\n",
    "    y_test = Y[sample]\n",
    "    y_train = np.delete(Y, sample, axis=0)\n",
    "    x_test = X[sample]\n",
    "    x_train = np.delete(X, sample, axis=0)\n",
    "    y_test = np_utils.to_categorical(y_test, 121)\n",
    "    y_train = np_utils.to_categorical(y_train, 121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image generator for altering images to capture more possibilities of plankton\n",
    "datagen = ImageDataGenerator(\n",
    "    data_format='channels_first',\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 1 Normal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=input_shape)\n",
    "\n",
    "if split_test_train:\n",
    "    hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                            steps_per_epoch=len(x_train) / batch_size, epochs=num_epochs)\n",
    "    score = model.evaluate(x_test, y_test, verbose=1)\n",
    "    \n",
    "else:\n",
    "    temp_x = X\n",
    "    temp_y = np_utils.to_categorical(Y, 121)\n",
    "    print(temp_y.shape)\n",
    "        \n",
    "    hist = model.fit_generator(datagen.flow(temp_x, temp_y, batch_size=batch_size),\n",
    "                                            steps_per_epoch=len(temp_y) / batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = cross_validation(parameters, X, Y, folds=num_folds, epochs=num_epochs, datagen=datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_image_names, submit_images = load_images(root + test_img_dir, shape=img_size)\n",
    "\n",
    "predicted_labels = model.predict(submit_images, verbose=1)\n",
    "predicted_labels = predicted_labels.argmax(axis=1).reshape(len(predicted_labels), 1)\n",
    "predictions = np.concatenate((submit_image_names, predicted_labels), axis=1)\n",
    "\n",
    "with open(submission_csv, 'w') as outpt:\n",
    "    writer = csv.writer(outpt)\n",
    "    writer.writerow(['image', 'class'])\n",
    "    np.apply_along_axis(writer.writerow, 1, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
